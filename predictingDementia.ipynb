{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#A-machine-learning-approach-to-help-detect-the-early-onset-of-dementia\" data-toc-modified-id=\"A-machine-learning-approach-to-help-detect-the-early-onset-of-dementia-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>A machine learning approach to help detect the early onset of dementia</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Wrangling\" data-toc-modified-id=\"Data-Wrangling-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Wrangling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-the-longnitudinal-dataset\" data-toc-modified-id=\"Loading-in-the-longnitudinal-dataset-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Loading in the longnitudinal dataset</a></span></li><li><span><a href=\"#Extracting-target-for-binary-classification\" data-toc-modified-id=\"Extracting-target-for-binary-classification-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Extracting target for binary classification</a></span></li><li><span><a href=\"#Investigating-data-structure\" data-toc-modified-id=\"Investigating-data-structure-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Investigating data structure</a></span></li><li><span><a href=\"#Remove-zero-variance-predictor\" data-toc-modified-id=\"Remove-zero-variance-predictor-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Remove zero-variance predictor</a></span></li><li><span><a href=\"#Split-dataset-into-train-and-test-sets\" data-toc-modified-id=\"Split-dataset-into-train-and-test-sets-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Split dataset into train and test sets</a></span></li></ul></li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exploratory data analysis</a></span></li><li><span><a href=\"#Data-pre-processing\" data-toc-modified-id=\"Data-pre-processing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracting-numerical-and-categorical-features\" data-toc-modified-id=\"Extracting-numerical-and-categorical-features-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Extracting numerical and categorical features</a></span></li><li><span><a href=\"#Logarithmic-transform\" data-toc-modified-id=\"Logarithmic-transform-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Logarithmic transform</a></span></li><li><span><a href=\"#Categorical-encoding\" data-toc-modified-id=\"Categorical-encoding-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Categorical encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#get_dummies-method\" data-toc-modified-id=\"get_dummies-method-1.3.3.1\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>get_dummies method</a></span></li></ul></li><li><span><a href=\"#Correlation-between-features\" data-toc-modified-id=\"Correlation-between-features-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Correlation between features</a></span></li><li><span><a href=\"#Hot-one-encoding\" data-toc-modified-id=\"Hot-one-encoding-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Hot-one encoding</a></span></li><li><span><a href=\"#Imputation\" data-toc-modified-id=\"Imputation-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Imputation</a></span></li><li><span><a href=\"#Investigating-and-dealing-with-outliers\" data-toc-modified-id=\"Investigating-and-dealing-with-outliers-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Investigating and dealing with outliers</a></span></li><li><span><a href=\"#Gathering-training-statistics\" data-toc-modified-id=\"Gathering-training-statistics-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Gathering training statistics</a></span></li><li><span><a href=\"#Drop-irrelevant-&amp;-collinear-features\" data-toc-modified-id=\"Drop-irrelevant-&amp;-collinear-features-1.3.9\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Drop irrelevant &amp; collinear features</a></span></li><li><span><a href=\"#Standardising-or-Normalising\" data-toc-modified-id=\"Standardising-or-Normalising-1.3.10\"><span class=\"toc-item-num\">1.3.10&nbsp;&nbsp;</span>Standardising or Normalising</a></span></li><li><span><a href=\"#Encode-target-group-to-numerical-values\" data-toc-modified-id=\"Encode-target-group-to-numerical-values-1.3.11\"><span class=\"toc-item-num\">1.3.11&nbsp;&nbsp;</span>Encode target group to numerical values</a></span></li><li><span><a href=\"#Shuffle-training-columns\" data-toc-modified-id=\"Shuffle-training-columns-1.3.12\"><span class=\"toc-item-num\">1.3.12&nbsp;&nbsp;</span>Shuffle training columns</a></span></li><li><span><a href=\"#Pipe-transform\" data-toc-modified-id=\"Pipe-transform-1.3.13\"><span class=\"toc-item-num\">1.3.13&nbsp;&nbsp;</span>Pipe transform</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Feature selection</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span></li></ul></li><li><span><a href=\"#Creating-a-web-GUI-to-predict-unseen-cases\" data-toc-modified-id=\"Creating-a-web-GUI-to-predict-unseen-cases-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Creating a web GUI to predict unseen cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Customising-.z.ws\" data-toc-modified-id=\"Customising-.z.ws-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Customising .z.ws</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overfitting-in-SVM\" data-toc-modified-id=\"Overfitting-in-SVM-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Overfitting in SVM</a></span></li><li><span><a href=\"#Using-Isolation-Forest-to-view-outliers-in-3D-space\" data-toc-modified-id=\"Using-Isolation-Forest-to-view-outliers-in-3D-space-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Using Isolation Forest to view outliers in 3D space</a></span></li><li><span><a href=\"#Machine-Learning-algorithms\" data-toc-modified-id=\"Machine-Learning-algorithms-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Machine Learning algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pros-vs-Cons\" data-toc-modified-id=\"Pros-vs-Cons-1.7.3.1\"><span class=\"toc-item-num\">1.7.3.1&nbsp;&nbsp;</span>Pros vs Cons</a></span></li></ul></li></ul></li><li><span><a href=\"#Sources\" data-toc-modified-id=\"Sources-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Sources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A machine learning approach to help detect the early onset of dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in all required source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\l code/q/init.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a port which will be accessible to a Web GUI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system\"p 9090\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the longnitudinal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is stored in a CSV, the standard kdb+ method of loading a CSV is used. \n",
    "The column names are transformed to conform with `camelCase` convention upon loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5#data:(camelCase each string cols data) xcol data:(\"**SJJSSJJJJFFFF\";enlist \",\")0:`:data/oasis_longitudinal_demographics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column header `group` conflicts with its reserved namesake function in the `.q` namespace. \n",
    "\n",
    "The internal .Q.id function is built for dealing with these cases - it will append a `1` to headers that conflict with reserved commands, as well as cleaning header characters that interfere with `select/exec/update` queries. \n",
    "\n",
    "However, a custom `renameCol` function allows a further degree of control by allowing the user to specify what the said column should be renamed as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`group in .Q.res,key `.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***group*** column is renamed to ***state***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameCol[`group`state;`data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting target for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state feature is extracted into a new `target` table and dropped from the main `data` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target:select state from data\n",
    "\n",
    "dropCol[`data;`state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info()` method in python provides a concise descriptive summary of a dataframe or table, mainly detailing datatypes, the existence of nulls within attributes and the total number of rows. This can be coded in q to provide the same effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the data is made up of 10 numerical columns and 4 remaining categorical columns. In machine learning these types of attributes are handled differently. Numerical features are easy to use, given most machine learning algorithms deal with numbers anyway, and generally these dont need transformed except during imputation and standardisation stages.  Categorical columns, however, will require modification through encoding and other additional measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient feature in python is the `describe()` method which returns a set of summary statistics for each column in a dataframe. Similarly, writing in q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe:{[t]\n",
    " //Extract non-numerical columns\n",
    " c:exec c from meta[t] where not t in \"ifj\";\n",
    " //Remove them from table\n",
    " t:![t;();0b;c];\n",
    " //Applying functions to each numerical column of table\n",
    " d:{`Field`Count`Mean`Min`Max`Median`q25`q75`STD`IQR!raze(x;(count; avg; min; max;med;pctl[.25;];pctl[.75;];dev;iqr)@\\:y[x])}[;t] each cols t;\n",
    " :d }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe[data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A few things to note: \n",
    "> - The first few columns are self-explantory and don't need any further elaboration. \n",
    "> - The q25 and q75 fields correspond to the percentiles - the value below which a cumulative sum of observations fall.\n",
    "> - The q25 value for age indicates that 25% of patients are younger than 71 years old.\n",
    "> - The q75 value for age illustrates that 75% of the dataset are younger than 82 years old.\n",
    "> - STD field is classified as the standard deviation for each column i.e. how dispersed the values are. \n",
    "> - IQR is the interquartile range.\n",
    "> - It's apparent that some fields have highly skewed distributions - namely `mrDelay` and `etiv`. It is recommended that before standardising or normalising, a log transformation be applied to make these distributions less skewed. This will aid in making the data more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove zero-variance predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hand` column has a single unique value. This is considered an **uninformative zero-variance** variable which can influence predictions. It is best practice to remove such features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCol[`data;`hand]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common oversight in many machine learning tasks is to perform exploratory data analysis and data preprocessing **prior** to splitting the dataset into train and test splits. This whitepaper will follow the general principle <sup>**[7]**</sup>:\n",
    "\n",
    "        Anything you learn, must be learned from the model's training data\n",
    "The test set is intended to gauge evaluator performance on totally unseen data. If it affects training in anyway, it can be classified as partially seen. If the full dataset is used during standardisation, the scaler has essentially *snooped* on data that should've been withheld from training and thus in the process has implicitly learned the mean,median and standard deviation of the testing data by including it in its calculations. This is called `data snooping bias`.  \n",
    "\n",
    "Resultantly, models will perform strongly on training data at the expense of generalising poorly on test data (a classic case of **overfitting**). \n",
    "\n",
    "To circumvent this possibility, the training and testing data is split, using a **80:20** ratio, *before* pre-processing steps are applied. The seed parameter ensures that the same indices will always be generated for every run to obtain reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTestSplit[data;target;0.2;seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape each `X_train`X_test`y_train`y_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of the target features in the y_train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc count'[group y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the target attribute exists a `Converted` state which signifies an individual being diagnosed with mild cognitive impairment on a follow-up visit, having initially shown no cognitive impairment prior.\n",
    "\n",
    "To keep this in the realms of a **binary classification** problem i.e whether an indiviudal is exhibiting early symptoms of Dementia or not, the `Converted` values **will be transformed** to `Demented` values.\n",
    "\n",
    "This step will be executed prior to evaluating machine learning algorithm perfomance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualising data graphically, it is possible to gain important insights that would not be as obvious through *eyeballing* the data. Exploratory data analysis (**EDA**) is the practice of describing data visually, through statistical and graphical means, to bring important patterns of that data into focus for further investigation.\n",
    "\n",
    "There are two main plot libraries in python to aid with visual analysis:\n",
    "\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "The seaborn library is preferred throughout this whitepaper mainly due to its aesthetics and ability to visualise many different features at once.\n",
    "\n",
    "Remaining honest, **test sets** are not used during visual analysis. \n",
    "\n",
    "To avoid any contamination with training data, a copy of the training data is created which will contain both X_train and y_train tables, joined together using the each-both adverb. The `train_copy` is deleted from memory after EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy:X_train,'y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init graph configuration settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphConfig[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many graphs in seaborn use the `hue` variable a  a setting parameter during plotting for colour encoding. In this example, hue is the target attribute `state`. Resultantly, seaborn will colour the datapoints different for each distinct state value (`Nondemented / Demented / Converted`). In the below example, *factor plots* portraying the distribution of nondemented and demented individuals in differing genders are coloured differently - light green signifying the non-demented state, dark green demented and torquoise represents converted patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorPlot[train_copy;`mF;`state];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Assumptions:\n",
    "- *Greater* ratio of *females to male*. \n",
    "- A larger number of male Dementia diagnoses compared to females.\n",
    "- Within the male class, there are more cases of Dementia than non-Dementia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a *count plot* is used to display the number of distinct values per column (similar to plotting histograms). \n",
    "\n",
    "A list of columns is split pairwise using the custom `splitList` function. Each column pair is then graphically displayed on the same axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "countPlot[train_copy;;] ./:  splitList[`age`ses`mmse`cdr`educ`visit;3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things can be concluded from these graphs about the dataset:\n",
    "\n",
    "- It seems that the majority of individuals in this study are aged  between 70 - 85 with the most frequent age being 73 years old. \n",
    "\n",
    "- More than half of the datasets' population received a CDR rating of 0 (>200 Subject IDs) while less than 10 subjects were diagnosed with a 2.0 CDR score.\n",
    "\n",
    "- The most frequent Socio-Economic-status score in this study was 2.0 with over > 100 subject IDs (103), followed by > 85 subject IDs graded as 1.0 on the ses scale.\n",
    "\n",
    "- Most of the individuals in this study undertook a significant amount of years in Education with over 2/3s of the study having at least studied 12+ years of Education.\n",
    "\n",
    "- The majority of Subject IDs scored highly in their MMSE examinations (> 50% scoring 29 or above).\n",
    "\n",
    "- The percentage ratio between females and males was 57:43."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*facet grid* plots are then computed to portray the variation of Alzheimers as a function of etiv,educ,ses,nwbv, mmse and asf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tnext stage of data visualization involves plotting FacetGrid charts\tto view\tthe\trelationship of\tDemented, Nondemented,Converted diagnoses against a\trange of different\tfeatures:\n",
    "1. *mmse*\t\n",
    "2. *ses*\n",
    "3. *nwbv*\t\n",
    "4. *etiv*\t\n",
    "5. *educ*\t\n",
    "6. *age*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facetGrid[train_copy;`state;\"Dementia Variation in \";] each `etiv`educ`ses`nwbv`mmse`asf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations:\n",
    " - Observing\tthe\tgraph \"*Dementia variation in mmse*\", for\thigh\tmmse\tscores\t(mmse=30 or\tmmse=29),\tin\tapproximately\t70%\t&\t55%\tof\tcases\trespectively,\tthe\tsubject\tIDs\tare\tgenerally\tconsidered as\tnon-demented. Finally,\tas\texpected, for low mmse\tscores\t(<22) all\tsubjects receive\ta\t'demented'\tdiagnosis. This\tmakes sense given that a mmse score\tof less\tthan 12\tindicates\tsevere dementia,\t13-20\tsuggests\tmoderate dementia\twhilst\t20-24\tsuggest\tmild\tsymptoms of\tDementia [8].Note,\tThere\tare some results where some\tsubjects score 27 yet are diagnosed as Demented which indicates that mmse score is not a certain indicator of Dementia symptoms. \n",
    "\n",
    "- Subjects\twho\texhibited\ta\tgreater\tpercentage\tof\tnormalized\twhole\tbrain volume,\thad\ta\tgreater\tchance of\treceiving\ta\t'Nondemented'\tdiagnosis.\t\tThe\tlower\tthe\tnWBV value,\tthe\tgreater\tthe\tchance\ta\tpatient\twas\tclassified\tas\t'demented'\tor\t'converted'. In\trecent years,\tbrain\tshrinkage\thas\tbecome\ta\tpotential important\tmarker\tfor\tearly changes\tin\tthe\tbrain\ttissue\twhere\tsuch\tsymptoms\thave\tbeen associated\tas\tearly\tmarkers\tfor\tailments\tof Alzheimer's\t[9].\n",
    " \n",
    " \n",
    "- Strangely,\tthere\tdidn't\tseem\tto\tbe\tmuch\tdifference\tbetween\tthe\tvariation\tof\tDementia\twith etiv\ti.e.\tthe\testimated\ttotal\tintracranial\tvolume\tdidn't\tvary\tmuch\tover\ttime\tand\ttherefore\tetiv\twas\tnot\tsignificantly\tdifferent\tbetween\tany\tof\tclassification\tgroups.\tAs\treferenced\tin the\tpaper **\"Inracranial\tVolume\tand\tAlzheimer\tDisease: Evidence\tagaisnt\tthe\tcerebal\tReserve\tHypothesis\"**\tby Jenkins\tet\tal <sup>**8**</sup>, the Mean TIV (*total intracranial volume*) did\tnot\tdiffer\tsignificantly\tbetween\tsubjects and\tthat\tthe\tonly\tsignificant\tpredictor\tof\tTIV\twas\tsex. Thus, it\twas\tconcluded\tthat\ttheir\tfindings do\tnot\tsuggest\ta\tlarger\tbrain\tprovides\tbetter\t'protection'\tagainst\tAD\t<sup>**8**</sup>. This\texplains the visual\t cues\tseen\tin\tthe \"Dementia variation in etiv\"\tplot i.e. 'Demented'\tand\t'Nondemented'\tsubjects\tshare relatively\tthe\t same\tetiv\tvalues.\n",
    "\n",
    "\n",
    "- It seems that Dementia is more prevalent in subjects who had fewer years in Education. For e.g. any subject who has participated in less than 13 years of Education had a slightly increased possibility of developing Dementia whereas the chances of having Dementia-like symptoms (on first visit) decreases quite sharply for subjects who have experienced 17+ years of Education.\n",
    "\n",
    "\n",
    "- Dementia is more prevalent in subjects who fall within the 65 - 85- year group. Before 65, one could hypothesise that the subject is too young for the full blown effects of Dementia to manifest. Similarly, after 85, given that a patient could have been living with the disease for many years, certain symptoms could have had a profound effect on an individual's mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting pairplots to visualise if relations exits among genders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairPlot[select asf,nwbv,etiv,mF from data; `mF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numerical and categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the training columns, by datatype, into their own group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols:exec c from meta[X_train] where not t in \"Cs\";\n",
    "strCols:exec c from meta[X_train] where t in \"C\";\n",
    "symCols:exec c from meta[X_train] where t in \"s\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the `describe` table there are some fields which are highly skewed and thus have the potential to alter performance. To circumvent this, a log-transform will be applied to each of the skewed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@[`X_train;`mrDelay`etiv;{log(1+x)}] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_dummies method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pivot table is computed to encode the categorical state variables - *Converted,Demented,Nondemented* - into binary `0 | 1` numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotTab[train_copy;`state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `lj` is then used in `dummies` to replace Converted,Demented,Nondemented values with the numeric values in the pivot table. The output is assigned as a global `dummy` table :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies:{[t;v;r]\n",
    "  pvt: t lj pivotTab[t;v];\n",
    "  t:(enlist v) _ pvt;\n",
    "  show 10#t;\n",
    "  @[`.;r;:;t]; }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies[train_copy;`state;`dummy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix is computed to gauge if any inherent relationships exist between the features and the **target** variable. The target `state` feature is converted into a dummy/indicator numeric variable to convey which attributes share the strongest correlation with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix[dummy];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `corrPairs` can also be called to display the **x** highest +ive correlation feature pairs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrPairs[dummy;`Demented`Nondemented`Converted;5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix, that depicts the pearson's r (cor coefficient) between two features, some assumptions can be hedged:\n",
    "\n",
    "- cdr has a strong positive correlation with both Converted, Demented and Nondemented. This is expected given that the CDR scale is used to gauge whether a patient has symptoms of Dementia. Given that this feature is strongly correlated with the target attribute, it would be advisable to drop this column during feature selection due to **collinearity**.\n",
    "\n",
    "- age and nwbv have strong negative correlations.\n",
    "\n",
    "- The educ feature has strong correlations with mmse, etiv and a very strong negative correlation with ses.\n",
    "\n",
    "- Most of the attributes share a correlation that is close to 0 thus we assume that there is little to no linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is used to transform/encode categorical values into separate binary values. The term *one-hot* signifies that there is `1` **Hot** value in the list, whilst remaining values `0` are **cold**.\n",
    "\n",
    "This is desired as most machine learning algorithms perform better when dealing with non-categorical values.\n",
    "\n",
    "Thus, **mF** categorical values are hot-one encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotEncode[`X_train;`mF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing features within machine learning is paramount. For various reasons, many real-world data sets come with missing values, often encoded as blanks, NaNs, or other placeholders. Many algorithms cannot work with missing features and thus they must be dealt with. There's varying options to combat this:\n",
    "\n",
    "- Delete the rows with  missing features - a method only advisable when there are enough training samples in the dataset. This can *introduce bias* and lead to a *loss of information and data* <sup>**7**</sup>.\n",
    "- Set the values to an ascertained value (mean, median or mode). This is an approximation which has the potential to add variance to the training set. \n",
    "- Using another machine learning algorithm to predict the missing values. This can introduce Bias and is only considered as a proxy for true values.\n",
    "\n",
    "The function `impute` takes a table reference and a `method` (*med, avg, mode*) for which to infer the missing values. \n",
    "\n",
    "Replacing **missing features** with the columns' **median value** is executed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute[`X_train;`med]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values within the **ses** column are reversed to convey the correct information to the model.\n",
    "\n",
    "Currently, a **low ses** value (*1 or 2*) represents a high economical status whilst a **high ses** value (4 or 5) depicts a low economical status. \n",
    "\n",
    "Values are reversed (<font color=blue> 1->5, 2->4, 4->2, 5->1</font>) so that the higher the score, the higher income an individual received:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update ses:{d:v!reverse v:asc[distinct X_train`ses];d[x]}'[ses] from `X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating and dealing with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An outlier table is defined to track any outlier values and their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers:([feature:()]index:();outlier:())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `outlierDetect` function uses the **z-score method** to detect if any datapoint has a *standard score greater than a threshold value* **3**. \n",
    "\n",
    "The z-score defines *how many standard deviations a datapoint is away from the mean*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierDetect[X_train;;`outliers] each numCols;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anamalous values are now persisted within the global `outliers` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outliers can be visualised by graphing whiskerplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiskerPlot select visit,mmse,cdr from X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Outliers can also be visualised in 3D space using the Isolation Forest technique. Please refer to the appendix: [Using Isolation Forest to view outliers in 3-D space ](#Using-Isolation-Forest-to-view-outliers-in-3D-space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with outliers is imperative in machine learning as they can significantly influence the data and thus add extra bias during evaluation. Some possible solutions to circumvent the effect of outliers:\n",
    "1. **Keep** the outliers in the dataset. \n",
    "    - If the dataset is small,which it is in this case, it might be *more costly to remove any rows* as **we lose information in terms of the variability in data** <sup>**9**</sup>.\n",
    "2. **Remove** the outlier rows from the training data.\n",
    "3. Apply a **log-transform** to each outlier.\n",
    "4. **Clip** each outlier to a given value such as the 5th or 95th percentile - a process called **Winsorization** <sup>**9**</sup>.\n",
    "\n",
    "\n",
    "The `outlierTransform` function is called below, with the **winsorize** transform so that each outlier value is replaced with the 5th and 95th percentile respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlierTransform[`winsorize;`outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering training statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training statistics table, `trainingStats`, is defined inside `mlf.q` to capture important metrics such as:\n",
    "- max,min column value\n",
    "- median,avg column value\n",
    "- standard deviation of a column\n",
    "\n",
    "These metrics will be used to transform unseen test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInfo[X_train;`trainingStats;] each  cols[X_train] except `subjectId`mriId;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant & collinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colinear and features are removed from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCol[`X_train;`subjectId`mriId`cdr];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising or Normalising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning it is generally a requirement that for algorithms to perform optimally, features should be standardised or normalised. \n",
    "\n",
    "\n",
    "When *standardising* a dataset, features are rescaled so that they have the properties of a standard normal distribution with\n",
    "\n",
    "$$μ = 0$$ $$σ = 1$$ \n",
    "\n",
    "where μ is the mean (average) and σ is the standard deviation from the mean i.e. they are centered around 0 with a std of 1 <sup>**10**</sup>.\n",
    "\n",
    "The **z-scores** of each datapoint can then be computed using:\n",
    "\n",
    "$$z  = \\frac{x-μ }{σ }$$\n",
    "\n",
    "Which can be written simply in `q` as:\n",
    "$$ q) stdScaler:{(x-avg x)%dev x} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to use *normalisation* (often called **min-max scaling**).\n",
    "\n",
    "When using this method, data is scaled to a fixed range - usually 0 to 1. The cost of having this fixed range is that we will end up with smaller standard deviations, which can suppress the effect of outliers <sup>**10**</sup>.\n",
    "\n",
    "**Min-Max scaling** is performed using the following equation:\n",
    "\n",
    "$$Xnorm  = \\frac{X-Xmin}{Xmax-Xmin }$$\n",
    "\n",
    "Which also can be written in q as:\n",
    "$$ q) minMaxScaler:{(x-m)%max[x]-m:min x} $$\n",
    "\n",
    "There's no obvious answer when choosing *standardisation or normalisation*.\n",
    "\n",
    "Both scaling transforms are executed on the training split and their outputs are visualised below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ".log.info[\"Applying stdScaler transformation:\";()];\n",
    "scaleBeforeAfter[stdScaler;X_train;`mmse`etiv`asf`age];\n",
    ".log.info[\"Applying minMax transformation:\";()];\n",
    "scaleBeforeAfter[minMaxScaler;X_train;`mmse`etiv`asf`age];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs, **Standardisation** is *chosen over* **normalisation**.\n",
    "\n",
    "This scaler is then permanently applied to the training dataset via a functional apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@[`X_train;cols X_train;stdScaler];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, several functions are added to the trainingStats table so that transformations can be easily reproduced on any dataset. These functions:\n",
    "- perform `feature scaling` by standardising or normalising numerical attributes that have different scales\n",
    "- perform `imputation` by replacing null/missing values with a median value. \n",
    "\n",
    "Each function is **lightweight** - a projection with computed training max,min,median,mean or standard deviation values.\n",
    "\n",
    "This means transforming unseen data becomes relatively simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update stdScaler:{(x-y)%z}[;first avgVal;first stdVal],\n",
    "       normaliser:{[x;minVal;maxVal] (x-minVal)%maxVal-minVal}[;first minVal;first maxVal],\n",
    "       imputer:{y^x}[;first medVal] by feature from `trainingStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, unseen visit values can be *standardised, normalised or imputed* referencing the below projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingStats[`visit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode target group to numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary classification problem, the `Converted` values in the y_train and y_test datasets are substituted to be `Demented`. In addition to this, these datasets are encoded, using a vector conditional, into numerical values where:\n",
    "\n",
    "`0` = `Nondemented`\n",
    "\n",
    "`1` = `Demented`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update state:?[state=`Nondemented;0;1] from `y_train;\n",
    "update state:?[state=`Nondemented;0;1] from `y_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding the target classes, the distribution of classes in the target dataset is checked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asc count'[group y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, there are 12 more `0` than `1` classes.\n",
    "\n",
    "Using a machine learning estimator out of the box when classes aren't evenly distributed can be problematic. \n",
    "\n",
    "To address this imbalanced class issue, new examples of the minority `1` class will be synthesised using the **SMOTE technique**. \n",
    "\n",
    "Smote (`Synthetic Minority Over-sampling`) connects the dots between minority classes, and along these connections, creates new synthetic minority classes. \n",
    "\n",
    "*This technique is applied **after** the kdb+ tables are transformed into python arrays.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle training columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of columns are shuffled pre-evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train:shuffle[cols X_train] xcols 0!X_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many transformation steps that need to be executed (**in the correct order**) before the data can be evaluated by machine learning algorithms. \n",
    "\n",
    "In Scikit-learn, a `pipeline` class is usually used to assist in sequences of transformations. \n",
    "\n",
    "To help automate the above machine learning workflow, feature engineering and data cleaning steps are grouped into a pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline:{[t;s;c]\n",
    " if[not -11h= type t;'\"Please pass table as reference\"];\n",
    " $[1< count skc:`mrDelay`etiv inter cols t; skc; skc:first skc];\n",
    " .log.info[\"Log transforming highly-skewed columns\";()];\n",
    " @[t;skc;{log(1+x)}];\n",
    " .log.info[\"Hot-encoding sym columns\";()];\n",
    " hotEncode[t;] each exec c from meta[t] where t in \"s\";\n",
    " .log.info[\"Dropping irrelevant features\";()];\n",
    " dropCol[t;c];\n",
    " `x1 set t;\n",
    " .log.info[\"Performing imputation and standard scaling on \",string[t];()];\n",
    " {[t;s;c;m] transform[t;s;c;m]}[t;s;cols t;] each `imputer`stdScaler;\n",
    " .log.info[\"Pipeline transform complete\";()];\n",
    " : 10#get t }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline[`X_test;trainingStats;`subjectId`mriId`mF`cdr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the X_train dataset, the order of columns are shuffled pre-evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test:shuffle[cols X_test] xcols 0!X_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kdb+ tables are then transformed into Python-readable matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab2Array each `X_train`y_train`X_test`y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, to perform the reversal transformation i.e. a python-array to kdb+ table, run the `array2Tab` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as alluded to already, the class imbalanced problem is addressed using the SMOTE technique to generate some minority `1` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm:smote[`k_neighbors pykw 5; `random_state pykw seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`X_train`y_train set' sm[`:fit_resample][X_train;y_train]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `No Free Lunch` theorem in Machine Learning stipulates that there is not one algorithm which works perfectly for all dataset <sup>**7**</sup>. Thus, the **performance of differing machine learning classifications** will be computed and compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main evaluation metric to gauge algorithm performance is the **AUC** (*Area under ROC curve*) **score.**\n",
    "\n",
    "The **ROC curve** displays the *trade-off* between the true positive rate and the false positive rate [6]. In the case for diagnosing Dementia, it’s imperative that patients who exhibit symptoms are identified as early as possible (high true positive rate) whilst healthy patients aren’t misdiagnosed with Dementia (low false positive rate) and begin treatment. **AUC is the most appropriate performance measure** as it will aid in distinguishing between the two diagnostic groups(Demented/ Nondemented) [7].\n",
    "\n",
    "Other evaluation metrics are used to compliment the AUC score but don’t carry the same weight. \n",
    "\n",
    "These include: \n",
    "1. **Cross-Validation score** (or gridsearch score).\n",
    "2. **Recall score** - The ratio of positive instances that each of our models detect. \n",
    "3. **Diagnostic odds ratio** (DOR score) is the odds of positivity in individuals with a illness relative to the odds in individuals without an illness <sup>**11**</sup>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluateAlgos` function calls `classReport` which computes all of the above evaluation metrics - AUC scores, classification reports (containing recall score) and a DOR score. This function will be called for a series of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluation, a global `scores` table is defined that will record important metrics that are used to evaluate the validity of each model and whether that model is using default parameters or hyperparameters that have been tuned via grid-search or randomized grid seach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores:([models:();parameters:()]DiagnosticOddsRatio:();TrainingAccuracy:();TestAccuracy:();TestAuc:())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*:\n",
    "- A **BaselineModel** is used as a **benchmark model**. *If algorithms perform better than this benchmark, it reaffirms that applying machine learning techniques to this dataset is applicable.*\n",
    "- Please refer to the appendix for some background on each algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear classifiers** are evaluated using *default* parameters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAlgos[linearClassifiers;`noTuning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **non-linear classifiers** with *default* parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAlgos[nonLinearClassifiers;`noTuning]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **tree based classifiers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAlgos[treeBasedClassifiers;`noTuning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`TestAuc xasc scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, most models with default parameters provided a significant improvement over the baseline model indicating that applying Machine learning techniques to this dataset is applicable.\n",
    "\n",
    "It is apparent that all models suffer from overfitting, a crux of having a small dataset. They score strongly in training predictions but generalise poorly on unseen data (**overfitting!**). \n",
    "\n",
    "> Please refer to follow section in the appendix to see the effects of [Overfitting in SVM](#Overfitting-in-SVM)\n",
    "\n",
    "The next steps will try and circumvent this issue by:\n",
    "> Using the **Boruta Algorithm** to remove irrelevant features and only retain features that fall within an area of absolute acceptance. \n",
    ">\n",
    "> Using the **GridSearch and RandomizedGridSearch** techniques with cross validation to finely tune hyperparameters that could unknowlingly exasperate overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the arrays back into q tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array2Tab each `X_train`X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Feature selection` is the process of finding a subset of features in the dataset `X` which have the greatest discriminatory power with respect to the target variable `y`. \n",
    "\n",
    "If feature selection is ignored:\n",
    "\n",
    "- It becomes **computationally expensive** as the model is processing a large number of features. \n",
    "\n",
    "- **garbage in, garbage out <sup>7</sup>.** When the number of features is significantly higher than optimal, a dip in accuracy is observed. `Occam's razor` stipulates that a problem should be simplified by removing irrelevant features that would introduce unncessary noise. If a model remembers noise in a small dataset, it could generalise poorly on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, instead of manually going through each feature to decide if any relationship exists between it and the target, an algorithm that is able to **autonomously** decide whether *any given feature of X bears some predictive value about y* is desired. \n",
    "\n",
    "This is what the **Boruta algorithm** does. \n",
    "\n",
    "A Random Forest algorithm is fitted on X and y. The feature importance is extracted from the RF model and only features that are **above a threshold of importance** are retained <sup>**12**</sup>. Digging deeper:\n",
    "\n",
    "- The **threshold** is defined as the *highest feature importance* that exists in the shadow features domain.\n",
    "\n",
    "- Shadow features are created by **randomly permuting** each column of X. So for a column `age:1 2 3 4 5`, the corresponding shadow feature is `shadow_age:3 5 2 4 1` where the *randomisation* is governed by the random seed.\n",
    "\n",
    "- Whenever a *feature* is **higher** than the *threshold* value, it's called a *hit* \n",
    "\n",
    "- The key to the boruta algorithm is **iteration**. Running the algorithm 100 times is more reliable than running it once. After the iteration operation has completed, features are grouped into 3 regions:\n",
    " - Area of **acceptance** - features that are deemed predictive and should be kept.\n",
    " - Area of **irresolution** - features that the algorithm are indecisive on.\n",
    " - Area of **rejection** - features that are not predictive and can be dropped <sup>**12**</sup>.\n",
    " \n",
    "This algorithm can be written in q as shown below.\n",
    "\n",
    "The iteration count is arbitrary. The user provides a list of integers where:\n",
    "- The iteration count is equal to the length of the list \n",
    "- Each value of the list is used as a random seed value\n",
    "\n",
    "So in the below case, *80* runs are executed against the training dataset *X_train* with the random seed value iterating each run (starting at 1, finishing at 80). The user decides how many features they extract from the area of acceptance (***3*** in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featSelect[X_train;y_train;1+til 80;3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features **[nwbv mmse educ]** are extracted and kept using the Boruta Algorithm.\n",
    "\n",
    "The remaining features are dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropCol[`X_train;cols[X_train] except borutaFeatures]\n",
    "dropCol[`X_test;cols[X_test] except borutaFeatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test sets are converted back to python arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab2Array each `X_train`X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to further improve the AUC score for each model, the hyperparameters for each classifier are optimized using one of the following techniques:\n",
    "1. **GridSearch**\n",
    "2. **RandomizedSearchCV**\n",
    "\n",
    "\n",
    "**GridSearch** simplifies hyperparameter implementation & optimization. A dictionary which contains the hyperparameters that need to be experimented are passed to a GridSearchCV function which will evaluate **all possible combinations** of hyperparameter values using cross validation on a model of the users choosing <sup>**7**</sup>. \n",
    "\n",
    "\n",
    "GridSearch is ideal when the combinations that you wish to explore aren’t plentiful, but whenever the hyperparameter space is large, it is preferable to make use of **RandomizedSearchCV**. This class works much like GridSearch but with one major difference – instead of experimenting on all possible parameter combinations, randomized search will evaluate a fixed number of hyperparameter combinations sampled from specified probability distributions <sup>**13**</sup>. It is generally preferred over GridSearch as the user has more control over the **computational budget** by setting the number of iterations <sup>**7**</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `optimalModels` key table is defined that will tabulate the optimal parameters found using the grid-search/randomized grid-search technique for a particular classifier. \n",
    "\n",
    "An *optimal* model will then be used by a web application to predict whether an individual is displaying alzheimer symptoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimalModels:([mdl:()]parameters:())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of **dictionaries** are defined that **represent the parameter space for each algorithm**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Random Forest parameter space\n",
    "rfParams:  (!) . flip(\n",
    " (`n_estimators; 15 25 30 35);\n",
    " (`min_samples_leaf; 1 + til 10);\n",
    " (`max_depth; 2 4 6);\n",
    " (`min_samples_split; 2 5 7 10 12);\n",
    " (`max_features; 2 3); \n",
    " (`criterion; `gini`entropy);\n",
    " (`class_weight; `balanced`balanced_subsample`None)) \n",
    "\n",
    "/Support vector machine parameter space\n",
    "svcParams:(!) . flip(\n",
    " (`kernel   ; (\"rbf\";\"linear\"));\n",
    " (`C        ; 0.0001 0.001 0.01 0.1 1);\n",
    " (`degree   ; 2 3 4); \n",
    " (`gamma    ; 0.0001 0.001 0.01 0.1 0.5));\n",
    "\n",
    "/Logistic regression parameter space\n",
    "lrParams: (!) . flip(\n",
    " (`C        ; 0.0001 0.001 0.01 0.1 1.0 10 100);\n",
    " (`max_iter ; 1000 5000 10000 );\n",
    " (`solver;     `liblinear`lbfgs);\n",
    " (`penalty  ; (\"l1\";\"l2\"))) \n",
    "\n",
    "/Decision tree parameter space\n",
    "dtParams: (!) . flip(\n",
    " (`max_leaf_nodes       ; 1+til 30);\n",
    " (`splitter             ; (\"random\";\"best\"));\n",
    " (`criterion            ; (\"gini\";\"entropy\"));\n",
    " (`max_depth            ; 1+til 10);\n",
    " (`min_samples_split    ; 0.1 0.2 0.3 0.5 0.6 0.7 0.8))\n",
    "\n",
    "/Gradient Boosting\n",
    "gbParams: (!) . flip(\n",
    " (`n_estimators      ; 500 1000 1500);\n",
    " (`learning_rate     ; 0.01 0.03 0.05 0.07);\n",
    " (`min_samples_split ; 2 4 6);\n",
    " (`min_samples_leaf  ; 3 5 7));\n",
    "\n",
    "adaParams: (!) . flip(\n",
    " (`n_estimators      ; 500 1000 1500 2000);\n",
    " (`learning_rate     ; 0.05 0.1 0.15 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary is defined to map each parameter space to its algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pspaces:(`SVM`LogisticRegression`DecisionTreeClassifier`Adaboost`GradientBoost`RandomForests)!(svcParams;lrParams;dtParams;adaParams;gbParams;rfParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f:hyperTune[;;`RandomizedSearchCV]\n",
    "/ Can also use the GridSearchCV optimizer to hypertune parameters - takes long, use threads (n_jobs)\n",
    "/ f:hyperTune[;;`GridSearchCV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eachKV[f] pspaces;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`TestAuc xdesc scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All models hypertuned** via RandomizedGridSearchCV computed **higher AUC scores** than the previous *no  tuning* evaluation, an indication that hyperparameter tuning via RandomizedSearchCV coupled with feature selection techniques, **improved performance significantly** (*the closer to **1**, the better*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`AucDiff xdesc \n",
    "  select BeforeAfter:TestAuc, AucDiff: abs .[-;TestAuc] by models from scores \n",
    "  where models in `SVM`LogisticRegression`DecisionTreeClassifier`Adaboost`GradientBoost`RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the highest AUC score is the **SVM** algo.\n",
    "\n",
    "The *SVM* model is relatively simple compared to the boosting & ensemble algorithms which suggest that these results reinforce the principle of `Occam's razor` where given a small dataset, the **application of simple models, with the fewest assumptions, yields the best results** <sup>**7**</sup>. \n",
    "\n",
    "Although, there is still some overfitting happening (`training acc > test acc`) mainly due to the size of this dataset, it is not as consequential as previous -  the training accuracies have decreased significantly whilst the test accuracies have risen substantially. \n",
    "\n",
    "In essence, the models aren't learning as many particulars in the training dataset and therefore generalising better on unseen data.\n",
    "\n",
    "Previously, models were learning details and noise in the training data to the extent that it was generalising very poorly on unseen data. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variable `modelsTrained` to true signalling to the GUI that the models have been trained and are ready to predict *Dementia* scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsTrained:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a web GUI to predict unseen cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since WebSockets are supported by most modern browsers, it is possible to create a HTML5 real-time web application (GUI) and connect it to this kdb+ process through JavaScript. \n",
    "\n",
    "> A detailed introduction to websockets can be found in the Kx whitepaper ***Kdb+ and WebSockets***:  https://code.kx.com/q/wp/websockets/.\n",
    "\n",
    "This simple GUI will serve as the entry-point for a potential client who wants to ascertain, by entering some data input, if an individual is exhibiting any symptoms of dementia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customising .z.ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, make sure some arbritary port is open so the web application can connect (should be port `9090` by default):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kdb+ has a separate message handler for WebSockets called `.z.ws`, meaning all incoming WebSocket messages will be processed by this function. \n",
    "\n",
    "Note, there is no default definition of .z.ws, it has been customised (*in this case*) to take biomarker/mri data and to return a result that indicates the possibility an individual is Demented (>0.5 - the closer to **1**, the greater the confidence a subject is demented). \n",
    "\n",
    "Breaking down each step of this custom `.z.ws` function. \n",
    "\n",
    "- The query sent by the client is passed as a  string for e..g \"(\\`a\\`b)!(40;50)\".\n",
    "- `value x` simply evaluates the string to create a dictionary. \n",
    "- The chosen algorithm to evaluate is extracted. If the `boruta` option has been enabled by the client, then every other feature, bar the significant features computed by the boruta algorithm, are dropped. If it has been disabled, all features bar `[mmse educ age]` are dropped.\n",
    "- The dictionary is then converted to a table. \n",
    "- The pipeline function is called which performs a litany of feature engineering steps. The table is then converted to a python-like array, ready for evaluation. \n",
    "- For the chosen algorithm, the optimal parameters found through GridSearch/RandomizedGridSearch, are used to train the algo. \n",
    "- The optimal model is then tested on the unseen data and returns a probability value of an individual being Nondemented / Demented. \n",
    "- As all messages passed through a WebSocket connection are asynchronous, the server response is handled by  `neg[.z.w]` which asynchronously pushes the results back to the handle which made the request. Just before, the results are pushed back `.j.j` is used to convert the kdb+ output to JSON. \n",
    "- The client can then parse the JSON into a JavaScript object upon arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".z.ws:{\n",
    " d:value x;\n",
    " //Check models have been evaluated\n",
    " if[not modelsTrained;\n",
    "    neg[.z.w] .j.j \"Models havent been trained yet. Cant predict a score!\";\n",
    "    :()\n",
    " ];\n",
    " //Get classifier from dict\n",
    " clf:d[`algo];\n",
    " //If boruta is enabled, use features computed by boruta algo, if not default to only using mmse,educ and age fields \n",
    " $[`Y=d`boruta;borutaFeatures:`nwbv`mmse`educ;borutaFeatures:`mmse`educ`age];\n",
    " //Drop algo and boruta fields from dict\n",
    " d:(`algo`boruta) _ d;\n",
    " //Change to table fmt and assign it to global t table\n",
    " `t set enlist d;\n",
    " //call pipeline fn to clean the dataset\n",
    " pipeline[`t;trainingStats;cols[t] except borutaFeatures];\n",
    " //Convert to python array\n",
    " tab2Array[`t];\n",
    " //get optimal parameters for classifier\n",
    " p:optimalModels[clf;`parameters];\n",
    " //configure new model using opt params\n",
    " m:algoMappings[clf][pykwargs p];\n",
    " //fit new model with training sets\n",
    " m[`:fit][X_train;y_train];\n",
    " //predict probability of subject being demented\n",
    " pred:m[`:predict_proba][t]`;\n",
    " res: raze pred;\n",
    " //convert to json and send output back to the handle which made the request\n",
    " neg[.z.w] .j.j \"F\"$.Q.fmt[5;3] res 1;\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer to the **README - Using the web GUI**  for example usage*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates that a kdb+ approach can be applied to a machine learning classification task to produce logical results. A multitude of techniques were reproduced in q to replicate their python equivalent. These included data wrangling & exploratory data analysis steps, feature engineering techniques and evaluation metrics (**AUC score, classification reports etc.**) to gauge model performance. \n",
    "\n",
    "Given the dataset was small, models trained on this set, ran the risk of overfitting where models were more susceptible to seeing patterns that didn't exist, resulting in *high variance and poor generalisation* on a test dataset. \n",
    "\n",
    "Thus, to circumvent the effect of overfitting, outliers were transformed and cross-validation techniques were applied.\n",
    "\n",
    "Initially, model performance was poor with simple models such as LinearDiscriminantAnalysis and LogisticRegression winning out (*complex models with many parameters increased the likelihood of overfitting*). \n",
    "\n",
    "To improve performance, feature selection coupled with grid search & RandomizedSearchCV techniques resulted in AUC score increasing by 20% across the board, with the best performing algorithm being the **Support vector machine** model.\n",
    "\n",
    "Finally, a HTML5 web application was created to serve as a entry-point for a potential user who wants to obtain, by entering some data fields, if an individual is showing any symptoms of dementia. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exhibit the effects of parameter values influencing overfitting, the **iris** dataset, one of the most renowned machine learning datasets, is used to demonstrate tuning the hyperparameters of a **support vector machine**.\n",
    "\n",
    "The hyperparameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel:`linear`rbf`poly\n",
    "C:0.01 0.1 1 10 100 1000\n",
    "gamma:0.01 1 10 100 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `iris dataset` is imported from sklearn's internal datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets:.p.import[`sklearn.datasets]\n",
    "iris:datasets[`:load_iris][]`;\n",
    "X:iris[`data;;0 1]\n",
    "Y:iris[`target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotSVC function allows **decision boundaries** in SVMs to be visualised for different parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSVC:{[title;model]\n",
    " x_min:-1+min first flip X;\n",
    " x_max:1+max first flip X;\n",
    " y_min:-1+min last flip X;\n",
    " y_max:1+max last flip X;\n",
    " h:(x_max%x_min)%100;\n",
    " x1:np[`:arange][x_min;x_max; h]`;\n",
    " y1:np[`:arange][y_min;y_max; h]`;\n",
    " b:(np[`:meshgrid][x1;y1]`);\n",
    " `xx`yy set' b;\n",
    " plt[`:subplot][1;1;1];\n",
    " z:model[`:predict][flip(raze xx;raze yy)];\n",
    " z1:164 208 # z`;\n",
    " plt[`:contourf][xx;yy;z1;`cmap pykw plt[`:cm][`:Paired];`alpha pykw 0.8];\n",
    " plt[`:scatter][X[;0]; X[;1]; `c pykw Y; `cmap pykw plt[`:cm][`:Paired]];\n",
    " plt[`:xlabel]\"Sepal length\";\n",
    " plt[`:ylabel]\"Sepal width\";\n",
    " plt[`:xlim][min first xx; max last xx];\n",
    " plt[`:title] title;\n",
    " plt[`:show][] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `overfit` function can be called to iterate over each hyperparameter value to exhibit the effects that different values have on parameters. \n",
    "\n",
    "The parameters investigated are:\n",
    "-  **C** : the penalty for misclassifying a data point.\n",
    "- **gamma**: can be thought of as the ‘spread’ of the kernel and therefore the decision region <sup>**14**</sup>.\n",
    "\n",
    "N.B. The effects of **C and gamma** parameters are studied with a **radial basis function kernel** (RBF) applied to a SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit:{[x;y]\n",
    "   title:string[x],\"=\",string[y];\n",
    "   $[x in `gamma`C; \n",
    "     mdl:svc[`kernel pykw `rbf; x pykw y][`:fit][X;Y];\n",
    "     mdl:svc[x pykw y][`:fit][X;Y]];\n",
    "   plotSVC[title;mdl];\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the decision boundaries for different **kernels**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit[`kernel;] each kernel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision boundaries for different **regularization values**(*C*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit[`C;] each C;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, comparing boundaries for different **gamma** values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit[`gamma;]each gamma;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Isolation Forest to view outliers in 3D space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method of detecting anomalies is using the `Isolation Forest` algorithm. It isolates outliers by selecting a random feature and then computing a split value between the max and min values of the said feature. This random partitioning results in shorter paths from anomalous data points which makes them distinguishable from other data points.\n",
    "\n",
    "These outliers are then visualised in 3-D space to further reinforce the influence these datapoints can exert on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier3Dplot[data;outliers;numCols];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros vs Cons\n",
    "\n",
    "`Logistic Regression:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Logistic Regression | Due\tto\tits\tsimplistic\tnature and\teasy implementation,it\tis\tan\tideal baseline\tfor\tany\tbinary\tclassification\tproblem. | This\tmodel\tis prone\tto\toverfitting\tand\tdoesn’t\tfare\twell\ton independent\tvariables that are\tin\tno\tway\tcorrelated with the\ttarget\tvariable. |\n",
    "| |Following\tthe\treasoning\tof\tOccam’s\tRazor,\tgiven\tthe\tsize\tof the\tMRI\tdataset,the application\tof\tsimple\tmodels\tcould\tyield\tthe\tbest\tresults. ||\n",
    "| |It\talso\tdoesn’t\trequire\tmuch computational\tpower\tand\tdoesn’t\trequire\tthe\tscaling\tof\tfeatures(quicker fitting\ttime) | |\n",
    "\n",
    "\n",
    "`Support vector machines:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| SVM | Can\thandle\tthe\tcase\twhere\tthe relationship\tthat\texists\tbetween\tthe features\tand\ttarget\tare\tnon-linear\t(kernel-trick) | Memory intensive |\n",
    "| | Has\tfew\thyperparameters\tto\ttune– in\tthe\tcase\tof this\tdataset,\tC\tand\tgamma\twill\tbe\tthe hyperparameters\tthat will need tuning\tas\tthe\t‘rbf’ kernel\twill\tbe\tused. | Can be long fitting times |\n",
    "\n",
    "\n",
    "`Naive Bayes:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Naive Bayes | The\ttraining\tset\tis\tsmall\tand\tthus high\tbias/low\tvariance\tclassifiers\t(i.e.\tNaïve bayes)\tshould\thave\tan advantage\tover\tcomplex models\twhich\tmay\thave\ta\ttendency\tto\toverfit. | Has trouble learning the interaction between different features |\n",
    "| |Extremely\tsimple\tto\timplement.||\n",
    "\n",
    "\n",
    "`Decision Trees:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Decision Trees | Decision\ttrees\trequire\tvery\tlittle data\tpreparation\ti.e.\tdon’t\trequire\t feature\tscaling.\tThey\tare\tthe\tfundamental\tconcept\tbehind the Random\tForest\tmodel | Main\tdisadvantage\tis\tthat an\tincrease\tin\tvariance\twhich leads\tto\tpoor\tgeneralization (tendency\tto\toverfit). |\n",
    "| | Decision\ttrees\tare\tfairly\tintuitive and\tthus their\tdecisions\tare\teasy to\tinterpret i.e. they\tprovide simple classification\trules\tthat can\tbe\tapplied\tmanually if\tneed be (known\tas\t‘White\tbox’ modelling). | Small variations\tin\tdata can\tresult\tin\tdifferent decision trees |\t\n",
    "\n",
    "`Random Forests:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Random Forests | The\trandom\tforest\tmodel is commonly referred\tto\tas\tthe\t‘Leatherman’\tof\tlearning methods and\tthus\tcan\tbe\tfitted\tto\tmost regression\tand\tclassification\ttasks.[20] | ‘Black\tbox’\tmodel\tthat is\tvery hard\tto\tinterpret\t(in\tcomparison\tto decision trees).|\n",
    "| | Although\tone\tshould\tundertake explicit\tefforts\tto avoid\toverfitting\t(cross\tvalidation\tetc) as\tnot every algorithm\tis\timmune\tto\toverfit, RF’s\tare\tless\tlikely\tto\toverfit\t| A\tlarge\tnumber\tof\ttrees\tmay make\tthe\tmodel\tslow\tdown when\tmaking\tpredictions\t|\n",
    "| | It\tcan\thandle\ta\tlarge\tnumber\tof\tfeatures\tand\tcan\thelp\testimate\twhich\tfeatures\tare\tparticularly important\tin\tthe\tunderlying\tdata | |\n",
    "\n",
    "\n",
    "`Adaboost:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Adaboost | It's simple to implement | Can be sensitive to noisy data or data which contains outliers |\n",
    "|| Not overly prone to overfitting ||\n",
    "\n",
    "`Gradient Boosting:`\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Gradient Boosting | Can produce high prediction accuracies | Computationally expensive |\n",
    "|| It can work on datasets that have missing features | It can cause overfitting due to the model trying to continually minimise all errors|\n",
    "|| Few preprocessing steps need to be implemented as it can handle both numerical and categorical data ||\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sources can be found in the accompanying **README.md** file*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "4.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
